---
permalink: /
title: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
Hello world! ðŸ‘‹ 
===
Iâ€™m Minwu, a graduate researcher at New York University Abu Dhabi, where Iâ€™m fortunate to be advised by Prof. [Keith Ross](https://sites.google.com/nyu.edu/keithross/). I also completed my undergraduate degree here, majoring in Computer Science. 

My research focuses on **LLM Reasoning**. Recently, I have been particularly interested in:

* Identifying and addressing the limitations of RLVR.
* RLVR in data-scarce settings.
* Generating synthetic data that better capture the human cognitive process during reasoning.
* Exploring test-time scaling methodologies that enable more effective reasoning and support longer context reasoning.

Before that, I worked in computational finance as well, particularly in corporate finance and macroeconomics.  

<!-- You can read more about my journey and why I pivoted to LLM research [here](/posts/unstructured/).   -->

For a detailed look at my background and experiences, feel free to check out my [CV](/files/MinwuKim_CV.pdf).  If you would like to talk about research or potential collaboration, feel free to ping me at *mwk300[at]nyu[dot]edu* :\)


News
-----
- 09.2025 One [work](https://arxiv.org/abs/2506.22638) is accepted at EMNLP 2025 BlackboxNLP Workshop!
- 08.2025: One [work](https://arxiv.org/abs/2505.13718) is accepted at EMNLP 2025 Main Conference! See you at Suzhou, China!
- 06.2025: Preprint for my new [paper](https://arxiv.org/abs/2506.22638) *Layer Importance for Mathematical Reasoning is Forged in Pre-Training and Invariant after Post-Training* is out!
- 05.2025: Preprint for my new [paper](https://arxiv.org/abs/2505.14216) *Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning* is out!
- 05.2025: Preprint for my new [paper](https://arxiv.org/abs/2505.13718) *Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings* is out!
- 02.2025: Preprint for my new [paper](https://arxiv.org/abs/2502.08680) *Mathematical Reasoning in Large Language Models: Assessing Logical and Arithmetic Errors across Wide Numerical Ranges* is out!
- 10.2024: My [paper](https://www.sciencedirect.com/science/article/pii/S240591882400031X) *Interpretable Machine Learning Model for Predicting Activist Investment Targets* is published at The Journal of Finance and Data Science!
- 09.2024: Started working as a graduate researcher at NYUAD!



Publications & Preprints
------
[5] Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning ([link](https://arxiv.org/abs/2505.14216))  
_Minwu Kim*, Anubhav Shrestha*, Safal Shrestha, Aadim Nepal, Keith Ross_  
Preprint

[4] Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings ([link](https://arxiv.org/abs/2505.13718))  
_Safal Shrestha, Minwu Kim, Aadim Nepal, Anubhav Shrestha, Keith Ross_  
EMNLP 2025 Main Conference. 



[3] Layer Importance for Mathematical Reasoning is Forged in Pre-Training and Invariant after Post-Training ([link](https://arxiv.org/abs/2506.22638))  
_Aadim Nepal, Safal Shrestha, Anubhav Shrestha, Minwu Kim, Keith Ross_   
Preprint. 

[2] Mathematical Reasoning in Large Language Models: Assessing Logical and Arithmetic Errors across Wide Numerical Ranges ([link](https://arxiv.org/abs/2502.08680))  
_Minwu Kim*, Safal Shrestha*, Keith Ross_  
Preprint.

[1] Interpretable Machine Learning Model for Predicting Activist Investment Targets ([link](https://www.sciencedirect.com/science/article/pii/S240591882400031X))  
_Minwu Kim*, Sidahmed Benabderrahmane, Talal Rahwan_  
The Journal of Finance and Data Science, 2024




<!-- Thoughts on the LLM
----

Having a machine that understands human language means a lot. I jotted down some thoughts about it. 

- **[LLM and Wittgenstein's Picture Theory](/posts/picture-theory/)**
  - The Picture Theory posits that the structure of the world parallels that of language. If a machine comprehends language, it is reasonable to consider it intelligent.
- **[Abilities of LLMs and Information Density in Language](/posts/information-density/)**
  - The capabilities of LLMs fundamentally stem from their training data. From this perspective, we can understand what LLMs excel at and where their limitations lie.
- **[From Structured to Unstructured - A Quantitative Breakthrough of Data](/posts/unstructured/)**
  - Between structured and unstructured data lies a tradeoff between the volume of information and its structure. LLMs, however, may serve as a breakthrough to overcome this challenge.
- **[Thoughts on Inference-time Compute](/posts/inference-time)**
  - Personal ideas on why and how to let the LLMs think more.

- **[OpenAI o1 & Recent Trend of LLMs](/posts/o1/)**
  - Some thoughts after playing with o1. -->


I Like
------
- Klein Blue
- IU
- Tottenham Hotspurs (Spursy who?)
- Tyler, the Creator
- Hermann Hesse
- Bitcoin

To know more about my taste, you are welcomed to visit my <a href="https://minwukim.net" target="_blank">personal website</a>
 *(Korean alert though)*
